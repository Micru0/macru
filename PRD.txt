Macru - Unified CAG App - Product Requirements Document (PRD)

1. Overview
	•	Product: A private, secure, modular AI-powered assistant (“Macru”) designed to be a central knowledge engine for an individual's work-related data.
	•	Problem Solved: Fragmented work information scattered across various documents, emails, calendars, and productivity tools leads to inefficiency and difficulty in accessing relevant context quickly. Standard AI assistants lack deep personalization, robust privacy controls for sensitive work data, and integration with specific workplace tools.
	•	Target Audience: Knowledge workers, professionals, researchers, or anyone dealing with a significant volume of digital information across different platforms who needs a unified, intelligent, and private way to access, query, and interact with their data.
	•	Value Proposition: The Unified CAG App provides a secure, encrypted, and personalized environment to ingest diverse data sources (uploads, email, calendar, Notion, etc.). It uses Cache-Augmented Generation (CAG) with powerful LLMs (like Gemini 2.5 Pro) and a modular memory layer (MEM0) to deliver high-context, relevant answers and insights based only on the user's private data. Its modular architecture allows for easy expansion and integration, ensuring long-term adaptability.
2. Core Features
	1	Document & Data Ingestion:
	◦	What: Securely ingest data from various sources. Initial focus on direct file uploads (PDF, DOCX, TXT). Future integrations planned for Gmail, Google Calendar, Notion, Google Drive, Linear, Figma, Mixpanel, Slack, etc., following a modular DataConnector pattern.
	◦	Why: Centralizes fragmented information, making it accessible for unified querying and analysis. The connector pattern ensures extensibility.
	◦	How: Users upload files via a dedicated UI. Future connectors will use secure API authentication (OAuth where applicable) to fetch data, transform it into a structured format, and store relevant metadata and content/embeddings securely.
	2	Cache-Augmented Generation (CAG) Querying:
	◦	What: Allow users to ask natural language questions against their ingested data, encompassing both unstructured text and structured metadata.
	◦	Why: Enables users to find information and synthesize insights across their entire knowledge base, querying text content (e.g., "summarize document X") and structured details (e.g., "what meetings do I have today?", "show me high-priority tasks from Linear") without manually searching individual sources. Leverages large language models for understanding and generation.
	◦	How:
	▪	Retrieval: When a query is received, identify relevant data chunks using techniques like vector search on embeddings. Simultaneously, query relevant structured metadata (e.g., using SQL filters on timestamps, statuses, types) stored alongside chunks or in related tables. Combine results from both retrieval methods.
	▪	Context Assembly: Efficiently assemble the retrieved text chunks and relevant structured data points to fit within the LLM's context window (e.g., Gemini 2.5 Pro's large window).
	▪	Generation: Pass the query and assembled context (containing both text and structured info) to the chosen LLM (via llmRouter.ts) for answer generation.
	▪	Caching: Implement caching for embeddings and potentially retrieved chunks/LLM responses to improve performance and reduce redundant processing/cost.
	3	Modular Memory Layer (MEM0):
	◦	What: A persistent, personalized, and encrypted memory store specific to each user.
	◦	Why: Enables the assistant to remember user preferences, key facts, context from previous interactions, and other user-specific information, leading to more personalized, efficient, and context-aware responses without the user needing to repeat themselves.
	◦	How:
	▪	Implemented initially using Supabase Postgres with strict Row Level Security (RLS) ensuring only the user can access their memory.
	▪	Sensitive data within the memory store will be encrypted.
	▪	The architecture will be modular (via memory.ts interface) to allow testing/swapping with other memory solutions later.
	▪	Relevant memory items are fetched and added to the LLM context dynamically during query processing.
	4	Multi-LLM Architecture:
	◦	What: Support for multiple underlying Large Language Models (LLMs).
	◦	Why: Provides flexibility, allows users to choose models based on cost/performance/preference, and future-proofs the application against changes in the LLM landscape.
	◦	How: Implemented via a routing layer (llmRouter.ts) that directs queries to the selected LLM API (e.g., Gemini, OpenAI GPT, Anthropic Claude, potentially local models via Ollama). Default set to Gemini 2.5 Pro. UI component (LLMSelector.tsx) allows user selection.
	5	Action Execution Layer (Optional):
	◦	What: Allow the LLM to propose and (optionally, with confirmation) execute actions on integrated platforms (e.g., create Notion page, schedule Google Calendar event, draft Gmail).
	◦	Why: Extends the assistant from purely informational to actively helpful in completing tasks.
	◦	How:
	▪	Implemented as an optional feature.
	▪	LLM generates action parameters based on user request.
	▪	Actions routed through a secure endpoint (api/action.ts) which performs auth checks and logging.
	▪	Crucially: User confirmation step is mandatory by default. A setting will allow disabling confirmation only for testing purposes, ideally restricted to sandboxed environments or with strong warnings.
	6	Secure Authentication & Data Storage:
	◦	What: Secure user authentication and encrypted storage for ingested data, metadata, and embeddings.
	◦	Why: Foundational for user trust and privacy, especially given the sensitive nature of work data.
	◦	How: Utilize Supabase Auth for user management. Supabase Storage for file uploads. Supabase Postgres for metadata, MEM0, and potentially vector embeddings (with encryption for sensitive fields).
3. User Experience
	1	User Personas:
	◦	The Researcher: Needs to synthesize information from many papers, documents, and notes. Values accurate retrieval and summarization across their entire corpus.
	◦	The Project Manager: Needs to track project status across documents, meeting notes (calendar), and task lists (Notion/Linear). Values quick access to relevant updates and context-aware summaries.
	◦	The Consultant: Deals with confidential client data across various formats. Values extreme privacy, security, and the ability to quickly reference information from past projects or client interactions (using MEM0).
	2	Key User Flows:
	◦	Onboarding: Sign up/Log in -> Initial welcome screen -> Optionally connect first data source (e.g., upload files).
	◦	Data Ingestion: Navigate to 'Connect' section -> Select data source (e.g., Upload, Notion) -> Authenticate (if needed) -> Initiate sync/upload -> View ingestion status.
	◦	Querying: Go to main query interface -> Type natural language question in QueryBox.tsx -> View response generated by LLM, including source attribution (if applicable) -> Ask follow-up questions.
	◦	Memory Interaction (Implicit): The system automatically retrieves relevant MEM0 context during querying. User can potentially view/manage non-sensitive parts of their memory via MemoryViewer.tsx.
	◦	Action Execution (Optional): User asks for an action (e.g., "Draft an email to Bob about the Q3 plan") -> System proposes action + parameters in UI -> User clicks 'Confirm' (default) -> System executes action via API.
	◦	Meeting Workflow Example (Requires Calendar Integration & Action Layer): User asks "Schedule a meeting with Alice tomorrow at 2 PM about Project Phoenix" -> System proposes `googleCalendar.createEvent` action -> User confirms -> Meeting is booked. Later, user asks "What meetings do I have today?" -> System queries ingested calendar data and lists meetings. User asks "Tell me about the Project Phoenix meeting" -> System identifies the meeting, retrieves related documents/tasks/emails from Notion, Linear, Gmail, etc. based on the meeting context/topic (potentially prioritizing based on relevance - basic relevance first, advanced topic prioritization later), and summarizes key information.
	◦	Settings Management: Access settings -> Select preferred LLM (LLMSelector.tsx) -> Manage data integrations -> Configure Action Layer settings (e.g., Confirmation On/Off - with warnings).
	3	UI/UX Considerations:
	◦	Clean, intuitive interface built with Next.js, Tailwind CSS, and shadcn/ui components.
	◦	Clear indication of data sources being queried.
	◦	Source attribution for answers derived from specific documents/data.
	◦	Clear status indicators for data ingestion and actions.
	◦	Emphasis on privacy controls and settings visibility.
	◦	Responsive design for different screen sizes.
	◦	Use of components listed in the original brief (Card, Textarea, Sidebar, Modals, etc.) for consistency.
4. Technical Architecture
	1	System Components:
	◦	Frontend: Next.js 14 + TypeScript application (/app, /components/ui). Handles UI rendering, user interactions, and communication with the backend API.
	◦	Backend API: Serverless functions (potentially via Next.js API routes or dedicated backend like Supabase Functions) (/api). Handles requests for querying, data ingestion triggers, memory management, action execution, and LLM routing.
	◦	Authentication: Supabase Auth.
	◦	Database: Supabase Postgres (for metadata, user info, MEM0, potentially vector indexes).
	◦	Storage: Supabase Storage (for uploaded files).
	◦	LLM Interface: llmRouter.ts in /lib connects to external LLM APIs (Gemini, OpenAI, Anthropic).
	◦	Memory Module: memory.ts in /lib provides an interface for interacting with the MEM0 store (initially backed by Supabase DB).
	◦	Data Connectors: Modular components (/api/integrations/*.ts) responsible for fetching and transforming data from specific sources.
	◦	Embedding Generation: A chosen model/service (could be via LLM API like Gemini, or dedicated embedding models) used during ingestion.
	◦	Vector Search: Initially potentially using pgvector in Supabase, or adaptable to a dedicated vector DB if needed.
	2	Data Models (High-Level):
	◦	Users: Standard user authentication model (provided by Supabase Auth).
	◦	DataSources: Records of connected sources (type, credentials/tokens, sync status).
	◦	Documents/Items: Metadata about ingested items (source, original ID, title, type, owner, timestamps).
	◦	Chunks: Processed chunks of text/data derived from items, linked to parent Document/Item.
	◦	Embeddings: Vector embeddings linked to Chunks.
	◦	Memory: User-specific key-value pairs or structured data (JSONB) in a secure table with RLS, linked to user ID.
	◦	ActionLogs: Records of proposed/executed actions.
	3	APIs and Integrations:
	◦	Internal: Frontend <-> Backend API for queries, data management, settings.
	◦	External:
	▪	LLM APIs (Google Gemini API, OpenAI API, Anthropic API).
	▪	Data Source APIs (Google APIs for Gmail/Calendar/Drive, Notion API, etc. - added incrementally).
	▪	Supabase APIs (Auth, DB, Storage).
	4	Infrastructure Requirements:
	◦	Hosting for Next.js application (e.g., Vercel, Netlify).
	◦	Supabase project (provides Auth, Postgres DB, Storage, Functions).
	◦	API keys/credentials for required LLMs and external data sources.
5. Development Roadmap
	•	Phase 1: Core Foundation & Querying (MVP)
	◦	Scaffolding: Set up Next.js project, TypeScript, Tailwind, shadcn/ui.
	◦	Authentication: Implement user sign-up/login using Supabase Auth.
	◦	Basic UI Shell: Create main layout, navigation (Sidebar), query input (QueryBox.tsx), response display area.
	◦	File Upload: Implement basic file upload (FileUploader.tsx) saving to Supabase Storage.
	◦	Ingestion Pipeline (Files Only): Basic document processing (text extraction for PDF, DOCX, TXT), chunking, embedding generation (using chosen model), storing metadata and embeddings in Supabase Postgres (potentially using pgvector).
	◦	CAG Querying (Files Only): Implement retrieval (vector search on Supabase), context assembly, call to one LLM (Gemini 2.5 Pro via llmRouter.ts), display response. Basic caching for embeddings.
	◦	Basic MEM0: Implement simple Supabase-backed memory store (memory.ts) with RLS and encryption for sensitive fields. Integrate fetching/prepending memory context to LLM calls. Basic UI (MemoryViewer.tsx) to view non-sensitive memory items.
	◦	LLM Selection UI: Implement LLMSelector.tsx and connect it to llmRouter.ts (even if only Gemini is fully integrated initially).
	•	Phase 2: Initial Integrations & Polish
	◦	First API Integration: Add support for one key external source (e.g., Notion OR Google Calendar/Gmail). Implement the DataConnector pattern, auth flow, data fetching, transformation, and integration into the ingestion pipeline.
	◦	Enhanced UI/UX: Improve response display (source attribution), error handling, loading states, settings page.
	◦	Refine CAG/Memory: Optimize retrieval, context assembly, and caching based on initial testing. Refine MEM0 storage and retrieval logic.
	•	Phase 3: Action Layer & Further Integrations
	◦	Action Execution (Optional): Implement the api/action.ts endpoint. Add UI for proposing actions and handling user confirmation (with the configurable setting and associated warnings/safety measures). Implement actions for the already integrated sources (e.g., create Notion page if Notion is integrated).
	◦	Additional API Integrations: Incrementally add support for other planned sources (Gmail/Calendar/Drive, etc.).
	•	Future Enhancements:
	◦	Advanced CAG/RAG optimizations (hybrid search, re-ranking).
	◦	More sophisticated MEM0 features (summarization, relationship extraction).
	◦	Support for local LLMs via Ollama.
	◦	Agent Protocol Integration (MCP, A2A).
	◦	Collaboration features (if applicable).
	◦	Advanced analytics/visualization of user data.
6. Logical Dependency Chain
	1	Foundation: Auth, Basic UI Shell, Core Supabase setup.
	2	Ingestion Path (Files): File Upload UI -> Storage -> Ingestion Pipeline (Processing, Chunking, Embedding) -> DB Storage. (Needs to be working before querying).
	3	Query Path (Files): Query Input -> Backend API -> Retrieval (needs Embeddings in DB) -> Context Assembly -> LLM Router (needs LLM API keys) -> LLM Call -> Response Display. (Needs Ingestion Path).
	4	Memory Integration: MEM0 DB Setup (RLS, Encryption) -> memory.ts interface -> Integration into Query Path (fetch memory before LLM call). (Can be built parallel to basic Query Path, but needs Auth).
	5	LLM Selection: UI Selector -> llmRouter.ts. (Can be built early, but only useful once multiple LLMs are integrated/tested).
	6	API Integrations: Each integration depends on the Foundation and builds upon the Ingestion Path concepts (Connector Pattern -> Auth -> Fetch -> Transform -> Ingest). Can be added incrementally after the core file query path works.
	7	Action Layer: Depends on Foundation, Query Path (for understanding intent), and specific API Integrations (to execute actions on those platforms). Should be built after the relevant integrations are stable.
Goal is to get a working end-to-end flow for File Upload -> Query -> Response as quickly as possible, then build MEM0, additional integrations, and actions on top.
7. Risks and Mitigations
	1	Technical Complexity (CAG/MEM0):
	◦	Risk: Efficiently implementing retrieval, large context handling, caching, and secure personalized memory can be challenging.
	◦	Mitigation: Start simple, leverage existing tools (pgvector), focus on modular design (memory.ts, llmRouter.ts), test thoroughly, and iterate. Rely on the large context of Gemini 2.5 Pro initially to simplify retrieval logic, but plan for more sophisticated RAG. Observe retrieval threshold: Keep an eye on the retrieval results (e.g., 'Found X relevant chunks') with the current threshold (e.g., 0.6). If it frequently misses relevant chunks or includes too many irrelevant ones, tuning will be required.
	2	Security & Privacy:
	◦	Risk: Handling sensitive user data requires robust security. Data leaks, unauthorized access, or insecure handling of API keys/actions could be catastrophic. MEM0 privacy is paramount. Action layer introduces risks.
	◦	Mitigation: Strict RLS on Supabase, encryption for sensitive data (MEM0, credentials), secure handling of API keys, mandatory user confirmation (default) for actions, strong warnings for disabling confirmation, regular security reviews, dependency scanning. Conduct testing in isolated environments.
	3	Scalability:
	◦	Risk: Performance might degrade as data volume (documents, chunks, embeddings) grows. Supabase pgvector might hit limits. LLM/API costs could escalate.
	◦	Mitigation: Monitor performance and cost. Optimize queries and caching. Design for potential migration to dedicated vector databases if needed. Implement cost control measures (e.g., user choice of LLM).
	4	API Dependencies & Rate Limits:
	◦	Risk: External APIs (LLMs, data sources) can change, have downtime, or enforce strict rate limits.
	◦	Mitigation: Implement robust error handling, retry logic (exponential backoff), monitoring for API interactions. Stay updated on API changes. Design connectors defensively.
	5	Scope Creep / MVP Definition:
	◦	Risk: The project is ambitious; trying to build too much in the initial phase could lead to delays or a lack of focus.
	◦	Mitigation: Strictly adhere to the phased roadmap. Focus Phase 1 relentlessly on the core file query loop + basic MEM0. Defer non-essential features and integrations. Make conscious decisions about what constitutes the minimal viable product.
	6	Database Schema Drift / Migration Conflicts:
	◦	Risk: Making direct database changes or altering functions (like Supabase Postgres functions) without first checking existing migrations (`supabase/migrations/`) can lead to conflicts, errors during deployment or local resets (`supabase db reset`), and inconsistencies between the codebase definition and the actual database state (as seen with the `match_documents` function needing updates).
	◦	Mitigation:
		*   **Development Environment Note:** This project does **not** use Docker Desktop or a local Supabase instance. All database schema changes (ALTER TABLE, CREATE FUNCTION, etc.) are applied **directly** via the Supabase Studio SQL Editor against the live development database.
		*   **Migration File Workflow:** ALWAYS check the `supabase/migrations/` directory for existing migrations related to the tables or functions being modified. If changes are needed, create a NEW migration file using `supabase migration new <description>`. Populate this file with the required SQL. Apply the changes manually via the Supabase Studio SQL Editor. This ensures schema changes are captured in version control even though local CLI commands like `db reset` or `migration up` are not used.
8. Appendix
	•	Initial Tech Stack Summary: Next.js 14, TypeScript, Tailwind CSS, shadcn/ui, Supabase (Auth, Postgres w/ pgvector, Storage), Gemini 2.5 Pro (default LLM).

